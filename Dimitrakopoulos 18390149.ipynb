{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Î’Î®Î¼Î± 1: Î•Î³ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Î±Ï€Î±ÏÎ±Î¯Ï„Î·Ï„Ï‰Î½ Ï€Î±ÎºÎ­Ï„Ï‰Î½**\n",
        "\n",
        "\n",
        "Î•Î³ÎºÎ±Î¸Î¹ÏƒÏ„Î¿ÏÎ¼Îµ Ï„Î¹Ï‚ Î±Ï€Î±ÏÎ±Î¯Ï„Î·Ï„ÎµÏ‚ Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎµÏ‚ Ï€Î¿Ï… Î¸Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎ¿Ï…Î¼Îµ Î³Î¹Î± Ï„Î·Î½ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± ÎºÎµÎ¹Î¼Î­Î½Î¿Ï… ÎºÎ±Î¹ Ï„Î·Î½ Ï…Î»Î¿Ï€Î¿Î¯Î·ÏƒÎ· Ï„Î·Ï‚ Î¼Î·Ï‡Î±Î½Î®Ï‚ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚. Î¤Î¿ nltk Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ Î³Î¹Î± Ï„Î·Î½ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Ï†Ï…ÏƒÎ¹ÎºÎ®Ï‚ Î³Î»ÏÏƒÏƒÎ±Ï‚, ÎµÎ½Ï pandas ÎºÎ±Î¹ json Î¸Î± Î¼Î±Ï‚ Î²Î¿Î·Î¸Î®ÏƒÎ¿Ï…Î½ ÏƒÏ„Î· Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½."
      ],
      "metadata": {
        "id": "FHuH_mP_AUJu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYRUkxsZ9Cxg",
        "outputId": "520076dd-3f6a-4f19-c811-f837f6895f8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk pandas scikit-learn rank-bm25 beautifulsoup4 requests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Î’Î®Î¼Î± 2: Î£Ï…Î»Î»Î¿Î³Î® Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î±Ï€ÏŒ Ï„Î· Wikipedia\n",
        "\n",
        "\n",
        "Î•Î´Ï Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Î­Î½Î±Î½ web scraper Ï€Î¿Ï… ÎµÎ¾Î¬Î³ÎµÎ¹ Î¬ÏÎ¸ÏÎ± Î±Ï€ÏŒ Ï„Î· Wikipedia. Î‘Î½Ï„Î¯ Î½Î± ÎµÎ¹ÏƒÎ¬Î³Î¿Ï…Î¼Îµ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï‡ÎµÎ¹ÏÎ¿ÎºÎ¯Î½Î·Ï„Î±, Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ Î­Î½Î±Î½ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î¿ Î¼Î·Ï‡Î±Î½Î¹ÏƒÎ¼ÏŒ Ï€Î¿Ï… ÏƒÏ…Î»Î»Î­Î³ÎµÎ¹ ÎºÎ±Î¹ Î±Ï€Î¿Î¸Î·ÎºÎµÏÎµÎ¹ Ï„Î± Î¬ÏÎ¸ÏÎ± ÏƒÎµ Î±ÏÏ‡ÎµÎ¯Î¿ JSON."
      ],
      "metadata": {
        "id": "SbTSNoNzAgb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Dimitrakopoulos Stylianos\n",
        "AM: 18390149\n",
        "Î ÏÎ¿Î³ÏÎ±Î¼Î¼Î± Î£Ï€Î¿Ï…Î´Ï‰Î½ Î Î‘Î”Î‘\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "def scrape_wikipedia_articles(url, num_articles=5):\n",
        "    base_url = \"https://en.wikipedia.org\"\n",
        "    articles = []\n",
        "    visited_urls = set()\n",
        "\n",
        "    def get_links(page_url):\n",
        "        \"\"\"Extract all Wikipedia article links from a given page.\"\"\"\n",
        "        response = requests.get(page_url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        links = []\n",
        "        for link in soup.find_all(\"a\", href=True):\n",
        "            href = link['href']\n",
        "            if href.startswith(\"/wiki/\") and \":\" not in href:  # Avoid non-article links\n",
        "                full_url = base_url + href\n",
        "                if full_url not in visited_urls:\n",
        "                    links.append(full_url)\n",
        "        return links\n",
        "\n",
        "    # Start with the main Wikipedia page\n",
        "    to_visit = [url]\n",
        "    while to_visit and len(articles) < num_articles:\n",
        "        current_url = to_visit.pop(0)\n",
        "        visited_urls.add(current_url)\n",
        "        try:\n",
        "            response = requests.get(current_url)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            title = soup.find(\"h1\").text.strip()\n",
        "            content = \"\\n\".join([p.text for p in soup.find_all(\"p\")])\n",
        "            if content:\n",
        "                articles.append({\"title\": title, \"url\": current_url, \"content\": content})\n",
        "                print(f\"Scraped: {title}\")\n",
        "            to_visit.extend(get_links(current_url))\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to scrape {current_url}: {e}\")\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Scrape articles and save to JSON\n",
        "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
        "articles = scrape_wikipedia_articles(url, num_articles=5)\n",
        "\n",
        "# Save to JSON file\n",
        "with open(\"wikipedia_articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(articles, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"Scraping complete. Data saved to wikipedia_articles.json.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ1udamw9kYD",
        "outputId": "5a5c3f0b-e809-4087-aee9-f26a9d1e0e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped: Main Page\n",
            "Scraped: Wikipedia\n",
            "Scraped: Free content\n",
            "Scraped: Encyclopedia\n",
            "Scraped: English language\n",
            "Scraping complete. Data saved to wikipedia_articles.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Î’Î®Î¼Î± 3: Î ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± ÎšÎµÎ¹Î¼Î­Î½Î¿Ï…**\n",
        "\n",
        "\n",
        "Î£Îµ Î±Ï…Ï„ÏŒ Ï„Î¿ ÏƒÏ„Î¬Î´Î¹Î¿, ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±Î¶ÏŒÎ¼Î±ÏƒÏ„Îµ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î¼Îµ Ï„ÎµÏ‡Î½Î¹ÎºÎ­Ï‚ ÏŒÏ€Ï‰Ï‚ tokenization, stemming, ÎºÎ±Î¹ stop-word removal. Î‘Ï…Ï„Î­Ï‚ Î¿Î¹ Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯ÎµÏ‚ Î¼Î±Ï‚ ÎµÏ€Î¹Ï„ÏÎ­Ï€Î¿Ï…Î½ Î½Î± Î¼ÎµÎ¹ÏÏƒÎ¿Ï…Î¼Îµ Ï„Î·Î½ Ï€Î¿Î»Ï…Ï€Î»Î¿ÎºÏŒÏ„Î·Ï„Î± Ï„Î¿Ï… ÎºÎµÎ¹Î¼Î­Î½Î¿Ï… ÎºÎ±Î¹ Î½Î± Î²ÎµÎ»Ï„Î¹ÏÏƒÎ¿Ï…Î¼Îµ Ï„Î·Î½ Ï€Î¿Î¹ÏŒÏ„Î·Ï„Î± Ï„Ï‰Î½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚."
      ],
      "metadata": {
        "id": "3JLGFMaJA-1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Dimitrakopoulos Stylianos\n",
        "AM: 18390149\n",
        "Î ÏÎ¿Î³ÏÎ±Î¼Î¼Î± Î£Ï€Î¿Ï…Î´Ï‰Î½ Î Î‘Î”Î‘\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Î¦oÏÏ„Ï‰ÏƒÎ· Ï„Ï‰Î½ Î¬ÏÎ¸ÏÏ‰Î½ Î±Ï€ÏŒ  JSON\n",
        "with open('wikipedia_articles.json', 'r', encoding='utf-8') as f:\n",
        "    articles_data = json.load(f)\n",
        "\n",
        "# ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ ÏƒÎµ DataFrame\n",
        "articles_df = pd.DataFrame(articles_data)\n",
        "\n",
        "# Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ· Î³Î¹Î± Ï„Î·Î½ ÎµÎ¾Î±Î³Ï‰Î³Î® Î»Î­Î¾ÎµÏ‰Î½ Î±Ï€ÏŒ Ï„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿\n",
        "def extract_tokens(text):\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return words\n",
        "\n",
        "# tokenization\n",
        "articles_df['tokens'] = articles_df['content'].apply(extract_tokens)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Î‘Ï†Î±Î¯ÏÎµÏƒÎ· stop words\n",
        "stop_words_set = set(stopwords.words('english'))\n",
        "articles_df['filtered_tokens'] = articles_df['tokens'].apply(\n",
        "    lambda words: [word for word in words if word not in stop_words_set]\n",
        ")\n",
        "\n",
        "# stemming\n",
        "stemmer = PorterStemmer()\n",
        "articles_df['stemmed_tokens'] = articles_df['filtered_tokens'].apply(\n",
        "    lambda words: [stemmer.stem(word) for word in words]\n",
        ")\n",
        "\n",
        "# Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î¿Ï… ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¼Î­Î½Î¿Ï… ÏƒÏ…Î½ÏŒÎ»Î¿Ï… Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ ÏƒÎµ CSV\n",
        "articles_df[['title', 'stemmed_tokens']].to_csv('processed_articles.csv', index=False)\n",
        "print(\"Processing complete. Data saved to 'processed_articles.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVP9yUQw-SXz",
        "outputId": "121165ca-fef9-4679-a7bc-89e2bf7aa139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing complete. Data saved to 'processed_articles.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Î’Î®Î¼Î± 4: Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î‘Î½Ï„ÎµÏƒÏ„ÏÎ±Î¼Î¼Î­Î½Î¿Ï… Î•Ï…ÏÎµÏ„Î·ÏÎ¯Î¿Ï… (Inverted Index)**\n",
        "\n",
        "\n",
        "Î— Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï„Î¿Ï… Î±Î½Ï„ÎµÏƒÏ„ÏÎ±Î¼Î¼Î­Î½Î¿Ï… ÎµÏ…ÏÎµÏ„Î·ÏÎ¯Î¿Ï… Î¼Î¬Ï‚ ÎµÏ€Î¹Ï„ÏÎ­Ï€ÎµÎ¹ Î½Î± ÎµÎ½Ï„Î¿Ï€Î¯Î¶Î¿Ï…Î¼Îµ Î³ÏÎ®Î³Î¿ÏÎ± Ï„Î± Î¬ÏÎ¸ÏÎ± Ï€Î¿Ï… Ï€ÎµÏÎ¹Î­Ï‡Î¿Ï…Î½ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î¿Ï…Ï‚ ÏŒÏÎ¿Ï…Ï‚. Î— Î´Î¿Î¼Î® Î±Ï…Ï„Î® Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ ÏƒÎµ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ­Ï‚ Î¼Î·Ï‡Î±Î½Î­Ï‚ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚ Î³Î¹Î± Î±Ï€Î¿Î´Î¿Ï„Î¹ÎºÎ® Î±Î½Î¬ÎºÏ„Î·ÏƒÎ· ÎµÎ³Î³ÏÎ¬Ï†Ï‰Î½."
      ],
      "metadata": {
        "id": "gYOl_BoGBHKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Dimitrakopoulos Stylianos\n",
        "AM: 18390149\n",
        "Î ÏÎ¿Î³ÏÎ±Î¼Î¼Î± Î£Ï€Î¿Ï…Î´Ï‰Î½ Î Î‘Î”Î‘\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import re\n",
        "\n",
        "# EÎ¾Î±Î³Ï‰Î³Î· Î»Î­Î¾ÎµÏ‰Î½ Î±Ï€ÏŒ Ï„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿\n",
        "def process_text(text):\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return words\n",
        "\n",
        "# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± inverted index\n",
        "def generate_index(data):\n",
        "    index = defaultdict(set)\n",
        "\n",
        "    for _, row in data.iterrows():\n",
        "        doc_title = row['title']\n",
        "        words = process_text(row['content'])\n",
        "\n",
        "        for word in words:\n",
        "            index[word].add(doc_title)\n",
        "\n",
        "    return {term: list(docs) for term, docs in index.items()}\n",
        "\n",
        "# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Î¿Ï… ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¼Î­Î½Î¿Ï… Î±ÏÏ‡ÎµÎ¯Î¿Ï…\n",
        "try:\n",
        "    articles_data = pd.read_csv(\"processed_articles.csv\")\n",
        "    articles_data['content'] = articles_data['stemmed_tokens'].apply(eval).apply(' '.join)\n",
        "    print(\"Processed data loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'processed_articles.csv' not found.\")\n",
        "    exit()\n",
        "\n",
        "# ÎšÎ±Ï„Î±ÏƒÎºÎµÏ…Î® ÎºÎ±Î¹ Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î¿Ï… inverted index\n",
        "print(\"Building the inverted index...\")\n",
        "inverted_index = generate_index(articles_data)\n",
        "print(\"Inverted index created.\")\n",
        "\n",
        "index_file = \"inverted_index.json\"\n",
        "with open(index_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(inverted_index, f, indent=4)\n",
        "\n",
        "print(f\"Inverted index saved to '{index_file}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkEir23x-iOb",
        "outputId": "d21a1650-4eb2-4543-9695-fce4e393e13d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed data loaded successfully.\n",
            "Building the inverted index...\n",
            "Inverted index created.\n",
            "Inverted index saved to 'inverted_index.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Î’Î®Î¼Î± 5: Î‘Î½Î¬ÎºÏ„Î·ÏƒÎ· Î Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯Î±Ï‚ (Retrieval Models)**\n",
        "\n",
        "\n",
        "Î£Îµ Î±Ï…Ï„ÏŒ Ï„Î¿ ÏƒÎ·Î¼ÎµÎ¯Î¿, ÎµÏ†Î±ÏÎ¼ÏŒÎ¶Î¿Ï…Î¼Îµ Ï„ÏÎµÎ¹Ï‚ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ Î¼ÎµÎ¸ÏŒÎ´Î¿Ï…Ï‚ Î±Î½Î¬ÎºÏ„Î·ÏƒÎ·Ï‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯Î±Ï‚:\n",
        "\n",
        "\n",
        "*   Boolean Retrieval\n",
        "*   TF-IDF\n",
        "*   BM25"
      ],
      "metadata": {
        "id": "84zeP7oNBR50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Dimitrakopoulos Stylianos\n",
        "AM: 18390149\n",
        "Î ÏÎ¿Î³ÏÎ±Î¼Î¼Î± Î£Ï€Î¿Ï…Î´Ï‰Î½ Î Î‘Î”Î‘\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np\n",
        "\n",
        "# Load ÎµÎ³Î³ÏÎ¬Ï†Ï‰Î½ Î±Ï€ÏŒ CSV\n",
        "def load_documents(file_path=\"processed_articles.csv\"):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        df['stemmed_tokens'] = df['stemmed_tokens'].apply(eval)\n",
        "        titles = df['title'].tolist()\n",
        "        documents = df['stemmed_tokens'].apply(lambda tokens: ' '.join(tokens)).tolist()\n",
        "        return titles, documents\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading documents: {e}\")\n",
        "        exit()\n",
        "\n",
        "# Load inverted index\n",
        "def load_inverted_index(file_path=\"inverted_index.json\"):\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: inverted_index.json not found.\")\n",
        "        exit()\n",
        "\n",
        "# Boolean Retrieval\n",
        "def boolean_search(query, inverted_index):\n",
        "    terms = query.split()\n",
        "    result_docs = set()\n",
        "\n",
        "    def get_docs(term):\n",
        "        return set(inverted_index.get(term, []))\n",
        "\n",
        "    current_docs = set()\n",
        "    operation = None\n",
        "\n",
        "    for term in terms:\n",
        "        if term.upper() in [\"AND\", \"OR\", \"NOT\"]:\n",
        "            operation = term.upper()\n",
        "        else:\n",
        "            docs = get_docs(term)\n",
        "            if operation == \"AND\":\n",
        "                current_docs &= docs\n",
        "            elif operation == \"OR\":\n",
        "                current_docs |= docs\n",
        "            elif operation == \"NOT\":\n",
        "                current_docs -= docs\n",
        "            else:\n",
        "                current_docs = docs\n",
        "\n",
        "    return current_docs\n",
        "\n",
        "# TF-IDF Retrieval\n",
        "def tfidf_retrieval(query, documents, inverted_index, titles):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    doc_vectors = vectorizer.fit_transform(documents)\n",
        "    query_vector = vectorizer.transform([query])\n",
        "\n",
        "    scores = (doc_vectors @ query_vector.T).toarray().flatten()\n",
        "    ranked_indices = np.argsort(scores)[::-1]\n",
        "\n",
        "    # Get Boolean filter\n",
        "    allowed_docs = boolean_search(query, inverted_index)\n",
        "\n",
        "    # Filter out disallowed documents\n",
        "    ranked_indices = [i for i in ranked_indices if titles[i] in allowed_docs]\n",
        "\n",
        "    return ranked_indices, scores[ranked_indices]\n",
        "\n",
        "# BM25 Retrieval\n",
        "def bm25_retrieval(query, documents, inverted_index, titles):\n",
        "    tokenized_docs = [doc.split() for doc in documents]\n",
        "    bm25 = BM25Okapi(tokenized_docs)\n",
        "    query_tokens = query.split()\n",
        "\n",
        "    scores = bm25.get_scores(query_tokens)\n",
        "    ranked_indices = np.argsort(scores)[::-1]\n",
        "\n",
        "    allowed_docs = boolean_search(query, inverted_index)\n",
        "\n",
        "\n",
        "    ranked_indices = [i for i in ranked_indices if titles[i] in allowed_docs]\n",
        "\n",
        "    return ranked_indices, scores[ranked_indices]\n"
      ],
      "metadata": {
        "id": "Q2WNE1pM-nhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Î’Î®Î¼Î± 6: Î Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î± Î‘Î½Î±Î¶Î·Ï„Î®ÏƒÎµÏ‰Î½**\n",
        "\n",
        "\n",
        "Î£Ï„Î¿ ÏƒÏ„Î¬Î´Î¹Î¿ Î±Ï…Ï„ÏŒ, Î· Î¼Î·Ï‡Î±Î½Î® Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚ Ï€Î±ÏÎ­Ï‡ÎµÎ¹ Î´ÏÎ¿ ÎµÏ€Î¹Î»Î¿Î³Î­Ï‚ ÏƒÏ„Î¿Î½ Ï‡ÏÎ®ÏƒÏ„Î·:\n",
        "\n",
        "\n",
        "\n",
        "*   Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Ï€ÏÎ¿ÎºÎ±Î¸Î¿ÏÎ¹ÏƒÎ¼Î­Î½Ï‰Î½ Î´Î¿ÎºÎ¹Î¼Î±ÏƒÏ„Î¹ÎºÏÎ½ Î±Î½Î±Î¶Î·Ï„Î®ÏƒÎµÏ‰Î½ â€“ Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ½Ï„Î±Î¹ Ï€ÏÎ¿ÎºÎ±Î¸Î¿ÏÎ¹ÏƒÎ¼Î­Î½Î± ÎµÏÏ‰Ï„Î®Î¼Î±Ï„Î± ÏÏƒÏ„Îµ Î½Î± Î±Î¾Î¹Î¿Î»Î¿Î³Î·Î¸ÎµÎ¯ Î· Î±ÎºÏÎ¯Î²ÎµÎ¹Î± ÎºÎ±Î¹ Î· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î±Ï„Î¹ÎºÏŒÏ„Î·Ï„Î± Ï„Ï‰Î½ Ï„ÏÎ¹ÏÎ½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Î±Î½Î¬ÎºÏ„Î·ÏƒÎ·Ï‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯Î±Ï‚ (Boolean Retrieval, TF-IDF, Okapi BM25).\n",
        "*   Î•Î¹ÏƒÎ±Î³Ï‰Î³Î® Ï€ÏÎ¿ÏƒÎ±ÏÎ¼Î¿ÏƒÎ¼Î­Î½Î·Ï‚ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚ â€“ ÎŸ Ï‡ÏÎ®ÏƒÏ„Î·Ï‚ Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÎµÏ€Î¹Î»Î­Î¾ÎµÎ¹ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î±Î½Î¬ÎºÏ„Î·ÏƒÎ·Ï‚ Ï€Î¿Ï… ÎµÏ€Î¹Î¸Ï…Î¼ÎµÎ¯ ÎºÎ±Î¹ Î½Î± ÎµÎ¹ÏƒÎ¬Î³ÎµÎ¹ Î´Î¹ÎºÏŒ Ï„Î¿Ï… ÎµÏÏÏ„Î·Î¼Î± Î³Î¹Î± Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ· ÏƒÏ„Î· ÏƒÏ…Î»Î»Î¿Î³Î® ÎµÎ³Î³ÏÎ¬Ï†Ï‰Î½.\n",
        "\n"
      ],
      "metadata": {
        "id": "5QbRICx1Bk8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Dimitrakopoulos Stylianos\n",
        "AM: 18390149\n",
        "Î ÏÎ¿Î³ÏÎ±Î¼Î¼Î± Î£Ï€Î¿Ï…Î´Ï‰Î½ Î Î‘Î”Î‘\n",
        "\"\"\"\n",
        "\n",
        "#from search_engine import load_documents, load_inverted_index, boolean_search, tfidf_retrieval, bm25_retrieval\n",
        "\n",
        "# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î±ÏÏ‡ÎµÎ¹Ï‰Î½\n",
        "titles, documents = load_documents(\"processed_articles.csv\")\n",
        "inverted_index = load_inverted_index(\"inverted_index.json\")\n",
        "\n",
        "# Test queries\n",
        "test_queries = [\n",
        "    {\"query\": \"maria\", \"expected\": {\"Main Page\"}},\n",
        "    {\"query\": \"maria AND trubnikova\", \"expected\": {\"Main Page\"}},\n",
        "    {\"query\": \"maria OR russian\", \"expected\": {\"Main Page\", \"Wikipedia\"}},\n",
        "    {\"query\": \"russian AND NOT maria\", \"expected\": {\"Wikipedia\"}},\n",
        "    {\"query\": \"NOT maria\", \"expected\": set()},  # Expect empty results\n",
        "    {\"query\": \"maria AND NOT russian\", \"expected\": set()},  # Expect empty if \"Main Page\" contains \"russian\"\n",
        "]\n",
        "\n",
        "# Function Î³Î¹Î± test ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ retrieval methods\n",
        "def run_tests(retrieval_function, method_name):\n",
        "    print(f\"\\nğŸ” Testing {method_name} Retrieval...\")\n",
        "\n",
        "    for test in test_queries:\n",
        "        query = test[\"query\"]\n",
        "        expected = test[\"expected\"]\n",
        "\n",
        "        print(f\"\\n[DEBUG] Running {method_name} Search for query: {query}\")\n",
        "\n",
        "        if method_name == \"Boolean\":\n",
        "            result = retrieval_function(query, inverted_index)\n",
        "        else:\n",
        "            ranked_indices, scores = retrieval_function(query, documents, inverted_index, titles)\n",
        "            result = {titles[idx] for idx in ranked_indices} if ranked_indices else set()\n",
        "\n",
        "        print(f\"ğŸ” Expected: {expected}\")\n",
        "        print(f\"âœ… {method_name} Search Result: {result}\")\n",
        "        print(f\"ğŸŸ¢ Pass: {result == expected}\\n\")\n",
        "\n",
        "# Function for custom user queries\n",
        "def run_custom_query():\n",
        "    print(\"\\nSelect a retrieval model:\")\n",
        "    print(\"1. Boolean Retrieval\")\n",
        "    print(\"2. TF-IDF Retrieval\")\n",
        "    print(\"3. Okapi BM25 Retrieval\")\n",
        "\n",
        "    choice = input(\"Enter your choice (1/2/3): \").strip()\n",
        "    model_mapping = {\"1\": (\"Boolean\", boolean_search), \"2\": (\"TF-IDF\", tfidf_retrieval), \"3\": (\"Okapi BM25\", bm25_retrieval)}\n",
        "\n",
        "    if choice not in model_mapping:\n",
        "        print(\"Invalid choice. Exiting...\")\n",
        "        return\n",
        "\n",
        "    model_name, model_function = model_mapping[choice]\n",
        "    query = input(\"\\nEnter your query: \").strip()\n",
        "\n",
        "    print(f\"\\nğŸ” Running {model_name} Retrieval for query: {query}\")\n",
        "\n",
        "    if model_name == \"Boolean\":\n",
        "        result = model_function(query, inverted_index)\n",
        "    else:\n",
        "        ranked_indices, scores = model_function(query, documents, inverted_index, titles)\n",
        "        result = {titles[idx] for idx in ranked_indices} if ranked_indices else set()\n",
        "\n",
        "    print(f\"ğŸ” {model_name} Search Result: {result}\\n\")\n",
        "\n",
        "# Main execution flow\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Would you like to run the predefined test queries or enter a custom query?\")\n",
        "    print(\"1. Run predefined test queries\")\n",
        "    print(\"2. Enter custom query\")\n",
        "\n",
        "    user_choice = input(\"Enter your choice (1/2): \").strip()\n",
        "\n",
        "    if user_choice == \"1\":\n",
        "        run_tests(boolean_search, \"Boolean\")\n",
        "        run_tests(tfidf_retrieval, \"TF-IDF\")\n",
        "        run_tests(bm25_retrieval, \"Okapi BM25\")\n",
        "    elif user_choice == \"2\":\n",
        "        run_custom_query()\n",
        "    else:\n",
        "        print(\"Invalid choice. Exiting...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqg8OqD2-rVo",
        "outputId": "010d55de-119a-42da-df6d-73ab91a64ff1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Would you like to run the predefined test queries or enter a custom query?\n",
            "1. Run predefined test queries\n",
            "2. Enter custom query\n",
            "Enter your choice (1/2): 1\n",
            "\n",
            "ğŸ” Testing Boolean Retrieval...\n",
            "\n",
            "[DEBUG] Running Boolean Search for query: maria\n",
            "ğŸ” Expected: {'Main Page'}\n",
            "âœ… Boolean Search Result: set()\n",
            "ğŸŸ¢ Pass: False\n",
            "\n",
            "\n",
            "[DEBUG] Running Boolean Search for query: maria AND trubnikova\n",
            "ğŸ” Expected: {'Main Page'}\n",
            "âœ… Boolean Search Result: set()\n",
            "ğŸŸ¢ Pass: False\n",
            "\n",
            "\n",
            "[DEBUG] Running Boolean Search for query: maria OR russian\n",
            "ğŸ” Expected: {'Main Page', 'Wikipedia'}\n",
            "âœ… Boolean Search Result: {'Wikipedia'}\n",
            "ğŸŸ¢ Pass: False\n",
            "\n",
            "\n",
            "[DEBUG] Running Boolean Search for query: russian AND NOT maria\n",
            "ğŸ” Expected: {'Wikipedia'}\n",
            "âœ… Boolean Search Result: {'Wikipedia'}\n",
            "ğŸŸ¢ Pass: True\n",
            "\n",
            "\n",
            "[DEBUG] Running Boolean Search for query: NOT maria\n",
            "ğŸ” Expected: set()\n",
            "âœ… Boolean Search Result: set()\n",
            "ğŸŸ¢ Pass: True\n",
            "\n",
            "\n",
            "[DEBUG] Running Boolean Search for query: maria AND NOT russian\n",
            "ğŸ” Expected: set()\n",
            "âœ… Boolean Search Result: set()\n",
            "ğŸŸ¢ Pass: True\n",
            "\n",
            "\n",
            "ğŸ” Testing TF-IDF Retrieval...\n",
            "\n",
            "[DEBUG] Running TF-IDF Search for query: maria\n",
            "ğŸ” Expected: {'Main Page'}\n",
            "âœ… TF-IDF Search Result: set()\n",
            "ğŸŸ¢ Pass: False\n",
            "\n",
            "\n",
            "[DEBUG] Running TF-IDF Search for query: maria AND trubnikova\n",
            "ğŸ” Expected: {'Main Page'}\n",
            "âœ… TF-IDF Search Result: set()\n",
            "ğŸŸ¢ Pass: False\n",
            "\n",
            "\n",
            "[DEBUG] Running TF-IDF Search for query: maria OR russian\n",
            "ğŸ” Expected: {'Main Page', 'Wikipedia'}\n",
            "âœ… TF-IDF Search Result: {'Wikipedia'}\n",
            "ğŸŸ¢ Pass: False\n",
            "\n",
            "\n",
            "[DEBUG] Running TF-IDF Search for query: russian AND NOT maria\n",
            "ğŸ” Expected: {'Wikipedia'}\n",
            "âœ… TF-IDF Search Result: {'Wikipedia'}\n",
            "ğŸŸ¢ Pass: True\n",
            "\n",
            "\n",
            "[DEBUG] Running TF-IDF Search for query: NOT maria\n",
            "ğŸ” Expected: set()\n",
            "âœ… TF-IDF Search Result: set()\n",
            "ğŸŸ¢ Pass: True\n",
            "\n",
            "\n",
            "[DEBUG] Running TF-IDF Search for query: maria AND NOT russian\n",
            "ğŸ” Expected: set()\n",
            "âœ… TF-IDF Search Result: set()\n",
            "ğŸŸ¢ Pass: True\n",
            "\n",
            "\n",
            "ğŸ” Testing Okapi BM25 Retrieval...\n",
            "\n",
            "[DEBUG] Running Okapi BM25 Search for query: maria\n",
            "ğŸ” Expected: {'Main Page'}\n",
            "âœ… Okapi BM25 Search Result: set()\n",
            "ğŸŸ¢ Pass: False\n",
            "\n",
            "\n",
            "[DEBUG] Running Okapi BM25 Search for query: maria AND trubnikova\n",
            "ğŸ” Expected: {'Main Page'}\n",
            "âœ… Okapi BM25 Search Result: set()\n",
            "ğŸŸ¢ Pass: False\n",
            "\n",
            "\n",
            "[DEBUG] Running Okapi BM25 Search for query: maria OR russian\n",
            "ğŸ” Expected: {'Main Page', 'Wikipedia'}\n",
            "âœ… Okapi BM25 Search Result: {'Wikipedia'}\n",
            "ğŸŸ¢ Pass: False\n",
            "\n",
            "\n",
            "[DEBUG] Running Okapi BM25 Search for query: russian AND NOT maria\n",
            "ğŸ” Expected: {'Wikipedia'}\n",
            "âœ… Okapi BM25 Search Result: {'Wikipedia'}\n",
            "ğŸŸ¢ Pass: True\n",
            "\n",
            "\n",
            "[DEBUG] Running Okapi BM25 Search for query: NOT maria\n",
            "ğŸ” Expected: set()\n",
            "âœ… Okapi BM25 Search Result: set()\n",
            "ğŸŸ¢ Pass: True\n",
            "\n",
            "\n",
            "[DEBUG] Running Okapi BM25 Search for query: maria AND NOT russian\n",
            "ğŸ” Expected: set()\n",
            "âœ… Okapi BM25 Search Result: set()\n",
            "ğŸŸ¢ Pass: True\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Dimitrakopoulos Stylianos\n",
        "AM: 18390149\n",
        "Î ÏÎ¿Î³ÏÎ±Î¼Î¼Î± Î£Ï€Î¿Ï…Î´Ï‰Î½ Î Î‘Î”Î‘\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "#from search_engine import load_documents, load_inverted_index, boolean_search, tfidf_retrieval, bm25_retrieval\n",
        "\n",
        "# Load necessary data\n",
        "titles, documents = load_documents(\"processed_articles.csv\")\n",
        "inverted_index = load_inverted_index(\"inverted_index.json\")\n",
        "\n",
        "# Define ground truth for evaluation\n",
        "ground_truth = {\n",
        "    \"maria\": {\"Main Page\"},\n",
        "    \"maria AND trubnikova\": {\"Main Page\"},\n",
        "    \"maria OR russian\": {\"Main Page\", \"Wikipedia\"},\n",
        "    \"russian AND NOT maria\": {\"Wikipedia\"},\n",
        "    \"NOT maria\": set(),\n",
        "    \"maria AND NOT russian\": set()\n",
        "}\n",
        "\n",
        "# Function to compute evaluation metrics\n",
        "def evaluate_retrieval(retrieval_function, method_name, is_boolean=False):\n",
        "    print(f\"\\nğŸ” Evaluating {method_name} Retrieval...\")\n",
        "\n",
        "    precision_scores, recall_scores, f1_scores = [], [], []\n",
        "\n",
        "    for query, expected in ground_truth.items():\n",
        "        print(f\"\\n[DEBUG] Running {method_name} for query: {query}\")\n",
        "\n",
        "        # Boolean retrieval returns a set, TF-IDF and BM25 return (ranked_indices, scores)\n",
        "        if is_boolean:\n",
        "            result = retrieval_function(query, inverted_index)\n",
        "        else:\n",
        "            ranked_indices, _ = retrieval_function(query, documents, inverted_index, titles)\n",
        "            result = {titles[idx] for idx in ranked_indices} if ranked_indices else set()\n",
        "\n",
        "        # Convert results and ground truth to binary format\n",
        "        all_documents = set(titles)\n",
        "        y_true = [1 if doc in expected else 0 for doc in all_documents]\n",
        "        y_pred = [1 if doc in result else 0 for doc in all_documents]\n",
        "\n",
        "        # Compute metrics\n",
        "        precision = precision_score(y_true, y_pred, zero_division=1)\n",
        "        recall = recall_score(y_true, y_pred, zero_division=1)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=1)\n",
        "\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        print(f\"ğŸ” Expected: {expected}\")\n",
        "        print(f\"âœ… {method_name} Search Result: {result}\")\n",
        "        print(f\"ğŸ¯ Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "    # Compute average scores\n",
        "    avg_precision = np.mean(precision_scores)\n",
        "    avg_recall = np.mean(recall_scores)\n",
        "    avg_f1 = np.mean(f1_scores)\n",
        "\n",
        "    print(f\"\\nğŸ“Š {method_name} Evaluation Summary:\")\n",
        "    print(f\"âš¡ Average Precision: {avg_precision:.4f}\")\n",
        "    print(f\"ğŸ“ˆ Average Recall: {avg_recall:.4f}\")\n",
        "    print(f\"ğŸ“Š Average F1-score: {avg_f1:.4f}\\n\")\n",
        "\n",
        "# Run evaluations\n",
        "if __name__ == \"__main__\":\n",
        "    evaluate_retrieval(boolean_search, \"Boolean\", is_boolean=True)\n",
        "    evaluate_retrieval(tfidf_retrieval, \"TF-IDF\")\n",
        "    evaluate_retrieval(bm25_retrieval, \"Okapi BM25\")\n"
      ],
      "metadata": {
        "id": "xsD1VWyd_BW5",
        "outputId": "4cdb1c8f-1888-4744-e88f-6e51e31a528a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ” Evaluating Boolean Retrieval...\n",
            "\n",
            "[DEBUG] Running Boolean for query: maria\n",
            "ğŸ” Expected: {'Main Page'}\n",
            "âœ… Boolean Search Result: set()\n",
            "ğŸ¯ Precision: 1.0000, Recall: 0.0000, F1-score: 0.0000\n",
            "\n",
            "[DEBUG] Running Boolean for query: maria AND trubnikova\n",
            "ğŸ” Expected: {'Main Page'}\n",
            "âœ… Boolean Search Result: set()\n",
            "ğŸ¯ Precision: 1.0000, Recall: 0.0000, F1-score: 0.0000\n",
            "\n",
            "[DEBUG] Running Boolean for query: maria OR russian\n",
            "ğŸ” Expected: {'Main Page', 'Wikipedia'}\n",
            "âœ… Boolean Search Result: {'Wikipedia'}\n",
            "ğŸ¯ Precision: 1.0000, Recall: 0.5000, F1-score: 0.6667\n",
            "\n",
            "[DEBUG] Running Boolean for query: russian AND NOT maria\n",
            "ğŸ” Expected: {'Wikipedia'}\n",
            "âœ… Boolean Search Result: {'Wikipedia'}\n",
            "ğŸ¯ Precision: 1.0000, Recall: 1.0000, F1-score: 1.0000\n",
            "\n",
            "[DEBUG] Running Boolean for query: NOT maria\n",
            "ğŸ” Expected: set()\n",
            "âœ… Boolean Search Result: set()\n",
            "ğŸ¯ Precision: 1.0000, Recall: 1.0000, F1-score: 1.0000\n",
            "\n",
            "[DEBUG] Running Boolean for query: maria AND NOT russian\n",
            "ğŸ” Expected: set()\n",
            "âœ… Boolean Search Result: set()\n",
            "ğŸ¯ Precision: 1.0000, Recall: 1.0000, F1-score: 1.0000\n",
            "\n",
            "ğŸ“Š Boolean Evaluation Summary:\n",
            "âš¡ Average Precision: 1.0000\n",
            "ğŸ“ˆ Average Recall: 0.5833\n",
            "ğŸ“Š Average F1-score: 0.6111\n",
            "\n",
            "\n",
            "ğŸ” Evaluating TF-IDF Retrieval...\n",
            "\n",
            "[DEBUG] Running TF-IDF for query: maria\n",
            "ğŸ” Expected: {'Main Page'}\n",
            "âœ… TF-IDF Search Result: set()\n",
            "ğŸ¯ Precision: 1.0000, Recall: 0.0000, F1-score: 0.0000\n",
            "\n",
            "[DEBUG] Running TF-IDF for query: maria AND trubnikova\n",
            "ğŸ” Expected: {'Main Page'}\n",
            "âœ… TF-IDF Search Result: set()\n",
            "ğŸ¯ Precision: 1.0000, Recall: 0.0000, F1-score: 0.0000\n",
            "\n",
            "[DEBUG] Running TF-IDF for query: maria OR russian\n",
            "ğŸ” Expected: {'Main Page', 'Wikipedia'}\n",
            "âœ… TF-IDF Search Result: {'Wikipedia'}\n",
            "ğŸ¯ Precision: 1.0000, Recall: 0.5000, F1-score: 0.6667\n",
            "\n",
            "[DEBUG] Running TF-IDF for query: russian AND NOT maria\n",
            "ğŸ” Expected: {'Wikipedia'}\n",
            "âœ… TF-IDF Search Result: {'Wikipedia'}\n",
            "ğŸ¯ Precision: 1.0000, Recall: 1.0000, F1-score: 1.0000\n",
            "\n",
            "[DEBUG] Running TF-IDF for query: NOT maria\n",
            "ğŸ” Expected: set()\n",
            "âœ… TF-IDF Search Result: set()\n",
            "ğŸ¯ Precision: 1.0000, Recall: 1.0000, F1-score: 1.0000\n",
            "\n",
            "[DEBUG] Running TF-IDF for query: maria AND NOT russian\n",
            "ğŸ” Expected: set()\n",
            "âœ… TF-IDF Search Result: set()\n",
            "ğŸ¯ Precision: 1.0000, Recall: 1.0000, F1-score: 1.0000\n",
            "\n",
            "ğŸ“Š TF-IDF Evaluation Summary:\n",
            "âš¡ Average Precision: 1.0000\n",
            "ğŸ“ˆ Average Recall: 0.5833\n",
            "ğŸ“Š Average F1-score: 0.6111\n",
            "\n",
            "\n",
            "ğŸ” Evaluating Okapi BM25 Retrieval...\n",
            "\n",
            "[DEBUG] Running Okapi BM25 for query: maria\n",
            "ğŸ” Expected: {'Main Page'}\n",
            "âœ… Okapi BM25 Search Result: set()\n",
            "ğŸ¯ Precision: 1.0000, Recall: 0.0000, F1-score: 0.0000\n",
            "\n",
            "[DEBUG] Running Okapi BM25 for query: maria AND trubnikova\n",
            "ğŸ” Expected: {'Main Page'}\n",
            "âœ… Okapi BM25 Search Result: set()\n",
            "ğŸ¯ Precision: 1.0000, Recall: 0.0000, F1-score: 0.0000\n",
            "\n",
            "[DEBUG] Running Okapi BM25 for query: maria OR russian\n",
            "ğŸ” Expected: {'Main Page', 'Wikipedia'}\n",
            "âœ… Okapi BM25 Search Result: {'Wikipedia'}\n",
            "ğŸ¯ Precision: 1.0000, Recall: 0.5000, F1-score: 0.6667\n",
            "\n",
            "[DEBUG] Running Okapi BM25 for query: russian AND NOT maria\n",
            "ğŸ” Expected: {'Wikipedia'}\n",
            "âœ… Okapi BM25 Search Result: {'Wikipedia'}\n",
            "ğŸ¯ Precision: 1.0000, Recall: 1.0000, F1-score: 1.0000\n",
            "\n",
            "[DEBUG] Running Okapi BM25 for query: NOT maria\n",
            "ğŸ” Expected: set()\n",
            "âœ… Okapi BM25 Search Result: set()\n",
            "ğŸ¯ Precision: 1.0000, Recall: 1.0000, F1-score: 1.0000\n",
            "\n",
            "[DEBUG] Running Okapi BM25 for query: maria AND NOT russian\n",
            "ğŸ” Expected: set()\n",
            "âœ… Okapi BM25 Search Result: set()\n",
            "ğŸ¯ Precision: 1.0000, Recall: 1.0000, F1-score: 1.0000\n",
            "\n",
            "ğŸ“Š Okapi BM25 Evaluation Summary:\n",
            "âš¡ Average Precision: 1.0000\n",
            "ğŸ“ˆ Average Recall: 0.5833\n",
            "ğŸ“Š Average F1-score: 0.6111\n",
            "\n"
          ]
        }
      ]
    }
  ]
}